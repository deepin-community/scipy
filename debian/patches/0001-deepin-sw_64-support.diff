diff --git a/scipy/_lib/boost/boost/atomic/detail/caps_arch_gcc_sw64.hpp b/scipy/_lib/boost/boost/atomic/detail/caps_arch_gcc_sw64.hpp
new file mode 100644
index 00000000..fb1c0035
--- /dev/null
+++ b/scipy/_lib/boost/boost/atomic/detail/caps_arch_gcc_sw64.hpp
@@ -0,0 +1,32 @@
+/*
+ * Distributed under the Boost Software License, Version 1.0.
+ * (See accompanying file LICENSE_1_0.txt or copy at
+ * http://www.boost.org/LICENSE_1_0.txt)
+ *
+ * Copyright (c) 2014 Miao Changwei, Uniontech.
+ */
+/*!
+ * \file   atomic/detail/caps_arch_gcc_sw64.hpp
+ *
+ * This header defines feature capabilities macros
+ */
+
+#ifndef BOOST_ATOMIC_DETAIL_CAPS_ARCH_GCC_SW64_HPP_INCLUDED_
+#define BOOST_ATOMIC_DETAIL_CAPS_ARCH_GCC_SW64_HPP_INCLUDED_
+
+#include <boost/atomic/detail/config.hpp>
+
+#ifdef BOOST_HAS_PRAGMA_ONCE
+#pragma once
+#endif
+
+#define BOOST_ATOMIC_INT8_LOCK_FREE 2
+#define BOOST_ATOMIC_INT16_LOCK_FREE 2
+#define BOOST_ATOMIC_INT32_LOCK_FREE 2
+#define BOOST_ATOMIC_INT64_LOCK_FREE 2
+#define BOOST_ATOMIC_POINTER_LOCK_FREE 2
+
+#define BOOST_ATOMIC_THREAD_FENCE 2
+#define BOOST_ATOMIC_SIGNAL_FENCE 2
+
+#endif // BOOST_ATOMIC_DETAIL_CAPS_ARCH_GCC_SW64_HPP_INCLUDED_
diff --git a/scipy/_lib/boost/boost/atomic/detail/core_arch_ops_gcc_sw64.hpp b/scipy/_lib/boost/boost/atomic/detail/core_arch_ops_gcc_sw64.hpp
new file mode 100644
index 00000000..1f702548
--- /dev/null
+++ b/scipy/_lib/boost/boost/atomic/detail/core_arch_ops_gcc_sw64.hpp
@@ -0,0 +1,937 @@
+/*
+ * Distributed under the Boost Software License, Version 1.0.
+ * (See accompanying file LICENSE_1_0.txt or copy at
+ * http://www.boost.org/LICENSE_1_0.txt)
+ *
+ * Copyright (c) 2014 Andrey Semashev
+ */
+/*!
+ * \file   atomic/detail/core_arch_ops_gcc_sw64.hpp
+ *
+ * This header contains implementation of the \c core_arch_operations template.
+ */
+
+#ifndef BOOST_ATOMIC_DETAIL_CORE_ARCH_OPS_GCC_SW64_HPP_INCLUDED_
+#define BOOST_ATOMIC_DETAIL_CORE_ARCH_OPS_GCC_SW64_HPP_INCLUDED_
+
+#include <cstddef>
+#include <boost/memory_order.hpp>
+#include <boost/atomic/detail/config.hpp>
+#include <boost/atomic/detail/storage_traits.hpp>
+#include <boost/atomic/detail/core_arch_operations_fwd.hpp>
+#include <boost/atomic/detail/header.hpp>
+
+#ifdef BOOST_HAS_PRAGMA_ONCE
+#pragma once
+#endif
+
+namespace boost {
+namespace atomics {
+namespace detail {
+
+/*
+    NB: The most natural thing would be to write the increment/decrement
+    operators along the following lines:
+
+    __asm__ __volatile__
+    (
+        "1: lldw %0,%2 \n"
+        "ldi %1, 1\n"
+        "wr_f %1\n"
+        "addw %0,1,%0 \n"
+        "lstw %0,%2 \n"
+        "rd_f %1\n"
+        "beq %1,1b\n"
+        : "=&r" (tmp), "=&r" (flag);
+        : "m" (value)
+        : "cc"
+    );
+*/
+
+struct core_arch_operations_gcc_sw64_base
+{
+    static BOOST_CONSTEXPR_OR_CONST bool full_cas_based = false;
+    static BOOST_CONSTEXPR_OR_CONST bool is_always_lock_free = true;
+
+    static BOOST_FORCEINLINE void fence_before(memory_order order) BOOST_NOEXCEPT
+    {
+        if ((static_cast< unsigned int >(order) & static_cast< unsigned int >(memory_order_release)) != 0u)
+            __asm__ __volatile__ ("memb" ::: "memory");
+    }
+
+    static BOOST_FORCEINLINE void fence_after(memory_order order) BOOST_NOEXCEPT
+    {
+        if ((static_cast< unsigned int >(order) & (static_cast< unsigned int >(memory_order_consume) | static_cast< unsigned int >(memory_order_acquire))) != 0u)
+            __asm__ __volatile__ ("memb" ::: "memory");
+    }
+
+    static BOOST_FORCEINLINE void fence_after_store(memory_order order) BOOST_NOEXCEPT
+    {
+        if (order == memory_order_seq_cst)
+            __asm__ __volatile__ ("memb" ::: "memory");
+    }
+};
+
+
+template< bool Signed, bool Interprocess >
+struct core_arch_operations< 4u, Signed, Interprocess > :
+    public core_arch_operations_gcc_sw64_base
+{
+    typedef typename storage_traits< 4u >::type storage_type;
+
+    static BOOST_CONSTEXPR_OR_CONST std::size_t storage_size = 4u;
+    static BOOST_CONSTEXPR_OR_CONST std::size_t storage_alignment = 4u;
+    static BOOST_CONSTEXPR_OR_CONST bool is_signed = Signed;
+    static BOOST_CONSTEXPR_OR_CONST bool is_interprocess = Interprocess;
+
+    static BOOST_FORCEINLINE void store(storage_type volatile& storage, storage_type v, memory_order order) BOOST_NOEXCEPT
+    {
+        fence_before(order);
+        storage = v;
+        fence_after_store(order);
+    }
+
+    static BOOST_FORCEINLINE storage_type load(storage_type const volatile& storage, memory_order order) BOOST_NOEXCEPT
+    {
+        storage_type v = storage;
+        fence_after(order);
+        return v;
+    }
+
+    static BOOST_FORCEINLINE storage_type exchange(storage_type volatile& storage, storage_type v, memory_order order) BOOST_NOEXCEPT
+    {
+        storage_type original, tmp, flag;
+        fence_before(order);
+        __asm__ __volatile__
+        (
+            "1:\n\t"
+            "mov %4, %1\n\t"
+            "lldw %0, %3\n\t"
+            "ldi %2, 1\n\t"
+            "wr_f %2\n\t"
+            "lstw %1, %3\n\t"
+            "rd_f %2\n\t"
+            "beq %2, 2f\n\t"
+
+            ".subsection 2\n\t"
+            "2: br 1b\n\t"
+            ".previous\n\t"
+
+            : "=&r" (original),  // %0
+              "=&r" (tmp),       // %1
+              "=&r" (flag)       // %2
+            : "m" (storage),     // %3
+              "r" (v)            // %4
+            : BOOST_ATOMIC_DETAIL_ASM_CLOBBER_CC
+        );
+        fence_after(order);
+        return original;
+    }
+
+    static BOOST_FORCEINLINE bool compare_exchange_weak(
+        storage_type volatile& storage, storage_type& expected, storage_type desired, memory_order success_order, memory_order failure_order) BOOST_NOEXCEPT
+    {
+        fence_before(success_order);
+        int success;
+        storage_type current;
+        __asm__ __volatile__
+        (
+            "1:\n\t"
+            "lldw %2, %4\n\t"                 // current = *(&storage)
+            "cmpeq %2, %0, %3\n\t"            // success = current == expected
+            "wr_f %3\n\t"                     // lock_flag = success
+            "mov %2, %0\n\t"                  // expected = current
+            "beq %3, 2f\n\t"                  // if (success == 0) goto end
+            "lstw %1, %4\n\t"                 // storage = desired
+            "rd_f %3\n\t"                     // success = lock_success
+            "2:\n\t"
+            : "+r" (expected),   // %0
+              "+r" (desired),    // %1
+              "=&r" (current),   // %2
+              "=&r" (success)    // %3
+            : "m" (storage)      // %4
+            : BOOST_ATOMIC_DETAIL_ASM_CLOBBER_CC
+        );
+        if (success)
+            fence_after(success_order);
+        else
+            fence_after(failure_order);
+        return !!success;
+    }
+
+    static BOOST_FORCEINLINE bool compare_exchange_strong(
+        storage_type volatile& storage, storage_type& expected, storage_type desired, memory_order success_order, memory_order failure_order) BOOST_NOEXCEPT
+    {
+        int success;
+        storage_type current, tmp;
+        fence_before(success_order);
+        __asm__ __volatile__
+        (
+            "1:\n\t"
+            "mov %5, %1\n\t"                  // tmp = desired
+            "ldlw %2, %4\n\t"                 // current = *(&storage)
+            "wr_f %3\n\t"                     // lock_flag = success
+            "cmpeq %2, %0, %3\n\t"            // success = current == expected
+            "mov %2, %0\n\t"                  // expected = current
+            "beq %3, 2f\n\t"                  // if (success == 0) goto end
+            "lstw %1, %4\n\t"                 // storage = tmp
+            "rd_f %3\n\t"                     // success = lock_success
+            "beq %3, 3f\n\t"                  // if (tmp == 0) goto retry
+            "2:\n\t"
+
+            ".subsection 2\n\t"
+            "3: br 1b\n\t"
+            ".previous\n\t"
+
+            : "+r" (expected),   // %0
+              "=&r" (tmp),       // %1
+              "=&r" (current),   // %2
+              "=&r" (success)    // %3
+            : "m" (storage),     // %4
+              "r" (desired)      // %5
+            : BOOST_ATOMIC_DETAIL_ASM_CLOBBER_CC
+        );
+        if (success)
+            fence_after(success_order);
+        else
+            fence_after(failure_order);
+        return !!success;
+    }
+
+    static BOOST_FORCEINLINE storage_type fetch_add(storage_type volatile& storage, storage_type v, memory_order order) BOOST_NOEXCEPT
+    {
+        storage_type original, modified, flag;
+        fence_before(order);
+        __asm__ __volatile__
+        (
+            "1:\n\t"
+            "lldw %0, %3\n\t"
+            "ldi %2, 1\n\t"
+            "wr_f %2\n\t"
+            "addw %0, %4, %1\n\t"
+            "lstw %1, %3\n\t"
+            "rd_f %2\n\t"
+            "beq %2, 2f\n\t"
+
+            ".subsection 2\n\t"
+            "2: br 1b\n\t"
+            ".previous\n\t"
+
+            : "=&r" (original),  // %0
+              "=&r" (modified),  // %1
+              "=&r" (flag)       // %2
+            : "m" (storage),     // %3
+              "r" (v)            // %4
+            : BOOST_ATOMIC_DETAIL_ASM_CLOBBER_CC
+        );
+        fence_after(order);
+        return original;
+    }
+
+    static BOOST_FORCEINLINE storage_type fetch_sub(storage_type volatile& storage, storage_type v, memory_order order) BOOST_NOEXCEPT
+    {
+        storage_type original, modified, flag;
+        fence_before(order);
+        __asm__ __volatile__
+        (
+            "1:\n\t"
+            "lldw %0, %3\n\t"
+            "ldi %2, 1\n\t"
+            "wr_f %2\n\t"
+            "subw %0, %4, %1\n\t"
+            "lstw %1, %3\n\t"
+            "rd_f %2\n\t"
+            "beq %2, 2f\n\t"
+
+            ".subsection 2\n\t"
+            "2: br 1b\n\t"
+            ".previous\n\t"
+
+            : "=&r" (original),  // %0
+              "=&r" (modified),  // %1
+              "=&r" (flag)       // %2
+            : "m" (storage),     // %3
+              "r" (v)            // %4
+            : BOOST_ATOMIC_DETAIL_ASM_CLOBBER_CC
+        );
+        fence_after(order);
+        return original;
+    }
+
+    static BOOST_FORCEINLINE storage_type fetch_and(storage_type volatile& storage, storage_type v, memory_order order) BOOST_NOEXCEPT
+    {
+        storage_type original, modified, flag;
+        fence_before(order);
+        __asm__ __volatile__
+        (
+            "1:\n\t"
+            "lldw %0, %3\n\t"
+            "ldi %2, 1\n\t"
+            "wr_f %2\n\t"
+            "and %0, %4, %1\n\t"
+            "lstw %1, %3\n\t"
+            "rd_f %2\n\t"
+            "beq %2, 2f\n\t"
+
+            ".subsection 2\n\t"
+            "2: br 1b\n\t"
+            ".previous\n\t"
+
+            : "=&r" (original),  // %0
+              "=&r" (modified),  // %1
+              "=&r" (flag)       // %2
+            : "m" (storage),     // %3
+              "r" (v)            // %4
+            : BOOST_ATOMIC_DETAIL_ASM_CLOBBER_CC
+        );
+        fence_after(order);
+        return original;
+    }
+
+    static BOOST_FORCEINLINE storage_type fetch_or(storage_type volatile& storage, storage_type v, memory_order order) BOOST_NOEXCEPT
+    {
+        storage_type original, modified, flag;
+        fence_before(order);
+        __asm__ __volatile__
+        (
+            "1:\n\t"
+            "lldw %0, %3\n\t"
+            "ldi %2, 1\n\t"
+            "wr_f %2\n\t"
+            "bis %0, %4, %1\n\t"
+            "lstw %1, %3\n\t"
+            "rd_f %2\n\t"
+            "beq %2, 2f\n\t"
+
+            ".subsection 2\n\t"
+            "2: br 1b\n\t"
+            ".previous\n\t"
+
+            : "=&r" (original),  // %0
+              "=&r" (modified),  // %1
+              "=&r" (flag)       // %2
+            : "m" (storage),     // %3
+              "r" (v)            // %4
+            : BOOST_ATOMIC_DETAIL_ASM_CLOBBER_CC
+        );
+        fence_after(order);
+        return original;
+    }
+
+    static BOOST_FORCEINLINE storage_type fetch_xor(storage_type volatile& storage, storage_type v, memory_order order) BOOST_NOEXCEPT
+    {
+        storage_type original, modified, flag;
+        fence_before(order);
+        __asm__ __volatile__
+        (
+            "1:\n\t"
+            "lldw %0, %2\n\t"
+            "ldi %2, 1\n\t"
+            "wr_f %2\n\t"
+            "xor %0, %3, %1\n\t"
+            "lstw %1, %2\n\t"
+            "rd_f %2\n\t"
+            "beq %2, 2f\n\t"
+
+            ".subsection 2\n\t"
+            "2: br 1b\n\t"
+            ".previous\n\t"
+
+            : "=&r" (original),  // %0
+              "=&r" (modified),  // %1
+              "=&r" (flag)       // %2
+            : "m" (storage),     // %3
+              "r" (v)            // %4
+            : BOOST_ATOMIC_DETAIL_ASM_CLOBBER_CC
+        );
+        fence_after(order);
+        return original;
+    }
+
+    static BOOST_FORCEINLINE bool test_and_set(storage_type volatile& storage, memory_order order) BOOST_NOEXCEPT
+    {
+        return !!exchange(storage, (storage_type)1, order);
+    }
+
+    static BOOST_FORCEINLINE void clear(storage_type volatile& storage, memory_order order) BOOST_NOEXCEPT
+    {
+        store(storage, 0, order);
+    }
+};
+
+
+template< bool Interprocess >
+struct core_arch_operations< 1u, false, Interprocess > :
+    public core_arch_operations< 4u, false, Interprocess >
+{
+    typedef core_arch_operations< 4u, false, Interprocess > base_type;
+    typedef typename base_type::storage_type storage_type;
+
+    static BOOST_FORCEINLINE storage_type fetch_add(storage_type volatile& storage, storage_type v, memory_order order) BOOST_NOEXCEPT
+    {
+        storage_type original, modified, flag;
+        base_type::fence_before(order);
+        __asm__ __volatile__
+        (
+            "1:\n\t"
+            "lldw %0, %3\n\t"
+            "ldi %2, 1\n\t"
+            "wr_f %2\n\t"
+            "addw %0, %4, %1\n\t"
+            "zapnot %1, 1, %1\n\t"
+            "lstw %1, %3\n\t"
+            "rd_f %2\n\t"
+            "beq %2, 2f\n\t"
+
+            ".subsection 2\n\t"
+            "2: br 1b\n\t"
+            ".previous\n\t"
+
+            : "=&r" (original),  // %0
+              "=&r" (modified),  // %1
+              "=&r" (flag)       // %2
+            : "m" (storage),     // %3
+              "r" (v)            // %4
+            : BOOST_ATOMIC_DETAIL_ASM_CLOBBER_CC
+        );
+        base_type::fence_after(order);
+        return original;
+    }
+
+    static BOOST_FORCEINLINE storage_type fetch_sub(storage_type volatile& storage, storage_type v, memory_order order) BOOST_NOEXCEPT
+    {
+        storage_type original, modified, flag;
+        base_type::fence_before(order);
+        __asm__ __volatile__
+        (
+            "1:\n\t"
+            "lldw %0, %3\n\t"
+            "ldi %2, 1\n\t"
+            "wr_f %2\n\t"
+            "subw %0, %4, %1\n\t"
+            "zapnot %1, 1, %1\n\t"
+            "lstw %1, %3\n\t"
+            "rd_f %2\n\t"
+            "beq %2, 2f\n\t"
+
+            ".subsection 2\n\t"
+            "2: br 1b\n\t"
+            ".previous\n\t"
+
+            : "=&r" (original),  // %0
+              "=&r" (modified),  // %1
+              "=&r" (flag)       // %2
+            : "m" (storage),     // %3
+              "r" (v)            // %4
+            : BOOST_ATOMIC_DETAIL_ASM_CLOBBER_CC
+        );
+        base_type::fence_after(order);
+        return original;
+    }
+};
+
+template< bool Interprocess >
+struct core_arch_operations< 1u, true, Interprocess > :
+    public core_arch_operations< 4u, true, Interprocess >
+{
+    typedef core_arch_operations< 4u, true, Interprocess > base_type;
+    typedef typename base_type::storage_type storage_type;
+
+    static BOOST_FORCEINLINE storage_type fetch_add(storage_type volatile& storage, storage_type v, memory_order order) BOOST_NOEXCEPT
+    {
+        storage_type original, modified, flag;
+        base_type::fence_before(order);
+        __asm__ __volatile__
+        (
+            "1:\n\t"
+            "lldw %0, %3\n\t"
+            "ldi %2, 1\n\t"
+            "wr_f %2\n\t"
+            "addw %0, %4, %1\n\t"
+            "sextb %1, %1\n\t"
+            "lstw %1, %3\n\t"
+            "rd_f %2\n\t"
+            "beq %2, 2f\n\t"
+
+            ".subsection 2\n\t"
+            "2: br 1b\n\t"
+            ".previous\n\t"
+
+            : "=&r" (original),  // %0
+              "=&r" (modified),  // %1
+              "=&r" (flag)       // %2
+            : "m" (storage),     // %3
+              "r" (v)            // %4
+            : BOOST_ATOMIC_DETAIL_ASM_CLOBBER_CC
+        );
+        base_type::fence_after(order);
+        return original;
+    }
+
+    static BOOST_FORCEINLINE storage_type fetch_sub(storage_type volatile& storage, storage_type v, memory_order order) BOOST_NOEXCEPT
+    {
+        storage_type original, modified, flag;
+        base_type::fence_before(order);
+        __asm__ __volatile__
+        (
+            "1:\n\t"
+            "lldw %0, %3\n\t"
+            "ldi %2, 1\n\t"
+            "wr_f %2\n\t"
+            "subw %0, %4, %1\n\t"
+            "sextb %1, %1\n\t"
+            "lstw %1, %3\n\t"
+            "rd_f %2\n\t"
+            "beq %2, 2f\n\t"
+
+            ".subsection 2\n\t"
+            "2: br 1b\n\t"
+            ".previous\n\t"
+
+            : "=&r" (original),  // %0
+              "=&r" (modified),  // %1
+              "=&r" (flag)       // %2
+            : "m" (storage),     // %3
+              "r" (v)            // %4
+            : BOOST_ATOMIC_DETAIL_ASM_CLOBBER_CC
+        );
+        base_type::fence_after(order);
+        return original;
+    }
+};
+
+
+template< bool Interprocess >
+struct core_arch_operations< 2u, false, Interprocess > :
+    public core_arch_operations< 4u, false, Interprocess >
+{
+    typedef core_arch_operations< 4u, false, Interprocess > base_type;
+    typedef typename base_type::storage_type storage_type;
+
+    static BOOST_FORCEINLINE storage_type fetch_add(storage_type volatile& storage, storage_type v, memory_order order) BOOST_NOEXCEPT
+    {
+        storage_type original, modified, flag;
+        base_type::fence_before(order);
+        __asm__ __volatile__
+        (
+            "1:\n\t"
+            "lldw %0, %3\n\t"
+            "ldi %2, 1\n\t"
+            "wr_f %2\n\t"
+            "addw %0, %4, %1\n\t"
+            "zapnot %1, 3, %1\n\t"
+            "lstw %1, %3\n\t"
+            "rd_f %2\n\t"
+            "beq %2, 2f\n\t"
+
+            ".subsection 2\n\t"
+            "2: br 1b\n\t"
+            ".previous\n\t"
+
+            : "=&r" (original),  // %0
+              "=&r" (modified),  // %1
+              "=&r" (flag)       // %2
+            : "m" (storage),     // %3
+              "r" (v)            // %4
+            : BOOST_ATOMIC_DETAIL_ASM_CLOBBER_CC
+        );
+        base_type::fence_after(order);
+        return original;
+    }
+
+    static BOOST_FORCEINLINE storage_type fetch_sub(storage_type volatile& storage, storage_type v, memory_order order) BOOST_NOEXCEPT
+    {
+        storage_type original, modified, flag;
+        base_type::fence_before(order);
+        __asm__ __volatile__
+        (
+            "1:\n\t"
+            "lldw %0, %3\n\t"
+            "ldi %2, 1\n\t"
+            "wr_f %2\n\t"
+            "subw %0, %4, %1\n\t"
+            "zapnot %1, 3, %1\n\t"
+            "lstw %1, %3\n\t"
+            "rd_f %2\n\t"
+            "beq %2, 2f\n\t"
+
+            ".subsection 2\n\t"
+            "2: br 1b\n\t"
+            ".previous\n\t"
+
+            : "=&r" (original),  // %0
+              "=&r" (modified),  // %1
+              "=&r" (flag)       // %2
+            : "m" (storage),     // %3
+              "r" (v)            // %4
+            : BOOST_ATOMIC_DETAIL_ASM_CLOBBER_CC
+        );
+        base_type::fence_after(order);
+        return original;
+    }
+};
+
+template< bool Interprocess >
+struct core_arch_operations< 2u, true, Interprocess > :
+    public core_arch_operations< 4u, true, Interprocess >
+{
+    typedef core_arch_operations< 4u, true, Interprocess > base_type;
+    typedef typename base_type::storage_type storage_type;
+
+    static BOOST_FORCEINLINE storage_type fetch_add(storage_type volatile& storage, storage_type v, memory_order order) BOOST_NOEXCEPT
+    {
+        storage_type original, modified, flag;
+        base_type::fence_before(order);
+        __asm__ __volatile__
+        (
+            "1:\n\t"
+            "lldw %0, %3\n\t"
+            "ldi %2, 1\n\t"
+            "wr_f %2\n\t"
+            "addw %0, %4, %1\n\t"
+            "sexth %1, %1\n\t"
+            "lstw %1, %3\n\t"
+            "rd_f %2\n\t"
+            "beq %2, 2f\n\t"
+
+            ".subsection 2\n\t"
+            "2: br 1b\n\t"
+            ".previous\n\t"
+
+            : "=&r" (original),  // %0
+              "=&r" (modified),  // %1
+              "=&r" (flag)       // %2
+            : "m" (storage),     // %3
+              "r" (v)            // %4
+            : BOOST_ATOMIC_DETAIL_ASM_CLOBBER_CC
+        );
+        base_type::fence_after(order);
+        return original;
+    }
+
+    static BOOST_FORCEINLINE storage_type fetch_sub(storage_type volatile& storage, storage_type v, memory_order order) BOOST_NOEXCEPT
+    {
+        storage_type original, modified, flag;
+        base_type::fence_before(order);
+        __asm__ __volatile__
+        (
+            "1:\n\t"
+            "lldw %0, %3\n\t"
+            "ldi %2, 1\n\t"
+            "wr_f %2\n\t"
+            "subw %0, %4, %1\n\t"
+            "sexth %1, %1\n\t"
+            "lstw %1, %3\n\t"
+            "rd_f %2\n\t"
+            "beq %2, 2f\n\t"
+
+            ".subsection 2\n\t"
+            "2: br 1b\n\t"
+            ".previous\n\t"
+
+            : "=&r" (original),  // %0
+              "=&r" (modified),  // %1
+              "=&r" (flag)       // %2
+            : "m" (storage),     // %3
+              "r" (v)            // %4
+            : BOOST_ATOMIC_DETAIL_ASM_CLOBBER_CC
+        );
+        base_type::fence_after(order);
+        return original;
+    }
+};
+
+
+template< bool Signed, bool Interprocess >
+struct core_arch_operations< 8u, Signed, Interprocess > :
+    public core_arch_operations_gcc_sw64_base
+{
+    typedef typename storage_traits< 8u >::type storage_type;
+
+    static BOOST_CONSTEXPR_OR_CONST std::size_t storage_size = 8u;
+    static BOOST_CONSTEXPR_OR_CONST std::size_t storage_alignment = 8u;
+    static BOOST_CONSTEXPR_OR_CONST bool is_signed = Signed;
+    static BOOST_CONSTEXPR_OR_CONST bool is_interprocess = Interprocess;
+
+    static BOOST_FORCEINLINE void store(storage_type volatile& storage, storage_type v, memory_order order) BOOST_NOEXCEPT
+    {
+        fence_before(order);
+        storage = v;
+        fence_after_store(order);
+    }
+
+    static BOOST_FORCEINLINE storage_type load(storage_type const volatile& storage, memory_order order) BOOST_NOEXCEPT
+    {
+        storage_type v = storage;
+        fence_after(order);
+        return v;
+    }
+
+    static BOOST_FORCEINLINE storage_type exchange(storage_type volatile& storage, storage_type v, memory_order order) BOOST_NOEXCEPT
+    {
+        storage_type original, tmp, flag;
+        fence_before(order);
+        __asm__ __volatile__
+        (
+            "1:\n\t"
+            "mov %4, %1\n\t"
+            "lldl %0, %3\n\t"
+            "ldi %2, 1\n\t"
+            "wr_f %2\n\t"
+            "lstl %1, %3\n\t"
+            "rd_f %2\n\t"
+            "beq %2, 2f\n\t"
+
+            ".subsection 2\n\t"
+            "2: br 1b\n\t"
+            ".previous\n\t"
+
+            : "=&r" (original),  // %0
+              "=&r" (tmp),       // %1
+              "=&r" (flag)       // %2
+            : "m" (storage),     // %3
+              "r" (v)            // %4
+            : BOOST_ATOMIC_DETAIL_ASM_CLOBBER_CC
+        );
+        fence_after(order);
+        return original;
+    }
+
+    static BOOST_FORCEINLINE bool compare_exchange_weak(
+        storage_type volatile& storage, storage_type& expected, storage_type desired, memory_order success_order, memory_order failure_order) BOOST_NOEXCEPT
+    {
+        fence_before(success_order);
+        int success;
+        storage_type current;
+        __asm__ __volatile__
+        (
+            "1:\n\t"
+            "lldl %2, %4\n\t"                 // current = *(&storage)
+            "cmpeq %2, %0, %3\n\t"            // success = current == expected
+            "wr_f %3\n\t"                     // lock_flag = success
+            "mov %2, %0\n\t"                  // expected = current
+            "beq %3, 2f\n\t"                  // if (success == 0) goto end
+            "lstl %1, %4\n\t"                 // storage = desired
+            "rd_f %3\n\t"                     // success = lock_success
+            "2:\n\t"
+            : "+r" (expected),   // %0
+              "+r" (desired),    // %1
+              "=&r" (current),   // %2
+              "=&r" (success)    // %3
+            : "m" (storage)      // %4
+            : BOOST_ATOMIC_DETAIL_ASM_CLOBBER_CC
+        );
+        if (success)
+            fence_after(success_order);
+        else
+            fence_after(failure_order);
+        return !!success;
+    }
+
+    static BOOST_FORCEINLINE bool compare_exchange_strong(
+        storage_type volatile& storage, storage_type& expected, storage_type desired, memory_order success_order, memory_order failure_order) BOOST_NOEXCEPT
+    {
+        int success;
+        storage_type current, tmp;
+        fence_before(success_order);
+        __asm__ __volatile__
+        (
+            "1:\n\t"
+            "mov %5, %1\n\t"                  // tmp = desired
+            "lldl %2, %4\n\t"                 // current = *(&storage)
+            "cmpeq %2, %0, %3\n\t"            // success = current == expected
+            "wr_f %3\n\t"                     // lock_flag = success
+            "mov %2, %0\n\t"                  // expected = current
+            "beq %3, 2f\n\t"                  // if (success == 0) goto end
+            "lstl %1, %4\n\t"                 // storage = tmp
+            "rd_f %3\n\t"                     // success = lock_success
+            "beq %3, 3f\n\t"                  // if (tmp == 0) goto retry
+            "2:\n\t"
+
+            ".subsection 2\n\t"
+            "3: br 1b\n\t"
+            ".previous\n\t"
+
+            : "+r" (expected),   // %0
+              "=&r" (tmp),       // %1
+              "=&r" (current),   // %2
+              "=&r" (success)    // %3
+            : "m" (storage),     // %4
+              "r" (desired)      // %5
+            : BOOST_ATOMIC_DETAIL_ASM_CLOBBER_CC
+        );
+        if (success)
+            fence_after(success_order);
+        else
+            fence_after(failure_order);
+        return !!success;
+    }
+
+    static BOOST_FORCEINLINE storage_type fetch_add(storage_type volatile& storage, storage_type v, memory_order order) BOOST_NOEXCEPT
+    {
+        storage_type original, modified, flag;
+        fence_before(order);
+        __asm__ __volatile__
+        (
+            "1:\n\t"
+            "lldl %0, %3\n\t"
+            "ldi %2, 1\n\t"
+            "wr_f %2\n\t"
+            "addl %0, %4, %1\n\t"
+            "lstl %1, %3\n\t"
+            "rd_f %2\n\t"
+            "beq %2, 2f\n\t"
+
+            ".subsection 2\n\t"
+            "2: br 1b\n\t"
+            ".previous\n\t"
+
+            : "=&r" (original),  // %0
+              "=&r" (modified),  // %1
+              "=&r" (flag)       // %2
+            : "m" (storage),     // %3
+              "r" (v)            // %4
+            : BOOST_ATOMIC_DETAIL_ASM_CLOBBER_CC
+        );
+        fence_after(order);
+        return original;
+    }
+
+    static BOOST_FORCEINLINE storage_type fetch_sub(storage_type volatile& storage, storage_type v, memory_order order) BOOST_NOEXCEPT
+    {
+        storage_type original, modified, flag;
+        fence_before(order);
+        __asm__ __volatile__
+        (
+            "1:\n\t"
+            "lldl %0, %3\n\t"
+            "ldi %2, 1\n\t"
+            "wr_f %2\n\t"
+            "subq %0, %4, %1\n\t"
+            "lstl %1, %3\n\t"
+            "rd_f %2\n\t"
+            "beq %2, 2f\n\t"
+
+            ".subsection 2\n\t"
+            "2: br 1b\n\t"
+            ".previous\n\t"
+
+            : "=&r" (original),  // %0
+              "=&r" (modified),  // %1
+              "=&r" (flag)       // %2
+            : "m" (storage),     // %3
+              "r" (v)            // %4
+            : BOOST_ATOMIC_DETAIL_ASM_CLOBBER_CC
+        );
+        fence_after(order);
+        return original;
+    }
+
+    static BOOST_FORCEINLINE storage_type fetch_and(storage_type volatile& storage, storage_type v, memory_order order) BOOST_NOEXCEPT
+    {
+        storage_type original, modified, flag;
+        fence_before(order);
+        __asm__ __volatile__
+        (
+            "1:\n\t"
+            "lldl %0, %3\n\t"
+            "ldi %2, 1\n\t"
+            "wr_f %2\n\t"
+            "and %0, %4, %1\n\t"
+            "lstl %1, %3\n\t"
+            "rd_f %2\n\t"
+            "beq %2, 2f\n\t"
+
+            ".subsection 2\n\t"
+            "2: br 1b\n\t"
+            ".previous\n\t"
+
+            : "=&r" (original),  // %0
+              "=&r" (modified),  // %1
+              "=&r" (flag)       // %2
+            : "m" (storage),     // %3
+              "r" (v)            // %4
+            : BOOST_ATOMIC_DETAIL_ASM_CLOBBER_CC
+        );
+        fence_after(order);
+        return original;
+    }
+
+    static BOOST_FORCEINLINE storage_type fetch_or(storage_type volatile& storage, storage_type v, memory_order order) BOOST_NOEXCEPT
+    {
+        storage_type original, modified, flag;
+        fence_before(order);
+        __asm__ __volatile__
+        (
+            "1:\n\t"
+            "lldl %0, %3\n\t"
+            "ldi %2, 1\n\t"
+            "wr_f %2\n\t"
+            "bis %0, %4, %1\n\t"
+            "lstl %1, %3\n\t"
+            "rd_f %2\n\t"
+            "beq %2, 2f\n\t"
+
+            ".subsection 2\n\t"
+            "2: br 1b\n\t"
+            ".previous\n\t"
+
+            : "=&r" (original),  // %0
+              "=&r" (modified),  // %1
+              "=&r" (flag)       // %2
+            : "m" (storage),     // %3
+              "r" (v)            // %4
+            : BOOST_ATOMIC_DETAIL_ASM_CLOBBER_CC
+        );
+        fence_after(order);
+        return original;
+    }
+
+    static BOOST_FORCEINLINE storage_type fetch_xor(storage_type volatile& storage, storage_type v, memory_order order) BOOST_NOEXCEPT
+    {
+        storage_type original, modified, flag;
+        fence_before(order);
+        __asm__ __volatile__
+        (
+            "1:\n\t"
+            "lldl %0, %3\n\t"
+            "ldi %2, 1\n\t"
+            "wr_f %2\n\t"
+            "xor %0, %4, %1\n\t"
+            "lstl %1, %3\n\t"
+            "rd_f %2\n\t"
+            "beq %2, 2f\n\t"
+
+            ".subsection 2\n\t"
+            "2: br 1b\n\t"
+            ".previous\n\t"
+
+            : "=&r" (original),  // %0
+              "=&r" (modified),  // %1
+              "=&r" (flag)       // %2
+            : "m" (storage),     // %3
+              "r" (v)            // %4
+            : BOOST_ATOMIC_DETAIL_ASM_CLOBBER_CC
+        );
+        fence_after(order);
+        return original;
+    }
+
+    static BOOST_FORCEINLINE bool test_and_set(storage_type volatile& storage, memory_order order) BOOST_NOEXCEPT
+    {
+        return !!exchange(storage, (storage_type)1, order);
+    }
+
+    static BOOST_FORCEINLINE void clear(storage_type volatile& storage, memory_order order) BOOST_NOEXCEPT
+    {
+        store(storage, (storage_type)0, order);
+    }
+};
+
+} // namespace detail
+} // namespace atomics
+} // namespace boost
+
+#include <boost/atomic/detail/footer.hpp>
+
+#endif // BOOST_ATOMIC_DETAIL_CORE_ARCH_OPS_GCC_SW64_HPP_INCLUDED_
diff --git a/scipy/_lib/boost/boost/atomic/detail/fence_arch_ops_gcc_sw64.hpp b/scipy/_lib/boost/boost/atomic/detail/fence_arch_ops_gcc_sw64.hpp
new file mode 100644
index 00000000..b17d679c
--- /dev/null
+++ b/scipy/_lib/boost/boost/atomic/detail/fence_arch_ops_gcc_sw64.hpp
@@ -0,0 +1,53 @@
+/*
+ * Distributed under the Boost Software License, Version 1.0.
+ * (See accompanying file LICENSE_1_0.txt or copy at
+ * http://www.boost.org/LICENSE_1_0.txt)
+ *
+ * Copyright (c) 2020 Andrey Semashev
+ */
+/*!
+ * \file   atomic/detail/fence_arch_ops_gcc_sw64.hpp
+ *
+ * This header contains implementation of the \c fence_arch_operations struct.
+ */
+
+#ifndef BOOST_ATOMIC_DETAIL_FENCE_ARCH_OPS_GCC_SW64_HPP_INCLUDED_
+#define BOOST_ATOMIC_DETAIL_FENCE_ARCH_OPS_GCC_SW64_HPP_INCLUDED_
+
+#include <boost/memory_order.hpp>
+#include <boost/atomic/detail/config.hpp>
+#include <boost/atomic/detail/header.hpp>
+
+#ifdef BOOST_HAS_PRAGMA_ONCE
+#pragma once
+#endif
+
+namespace boost {
+namespace atomics {
+namespace detail {
+
+//! Fence operations for Sunway
+struct fence_arch_operations_gcc_sw64
+{
+    static BOOST_FORCEINLINE void thread_fence(memory_order order) BOOST_NOEXCEPT
+    {
+        if (order != memory_order_relaxed)
+            __asm__ __volatile__ ("memb" ::: "memory");
+    }
+
+    static BOOST_FORCEINLINE void signal_fence(memory_order order) BOOST_NOEXCEPT
+    {
+        if (order != memory_order_relaxed)
+            __asm__ __volatile__ ("" ::: "memory");
+    }
+};
+
+typedef fence_arch_operations_gcc_sw64 fence_arch_operations;
+
+} // namespace detail
+} // namespace atomics
+} // namespace boost
+
+#include <boost/atomic/detail/footer.hpp>
+
+#endif // BOOST_ATOMIC_DETAIL_FENCE_ARCH_OPS_GCC_SW64_HPP_INCLUDED_
diff --git a/scipy/_lib/boost/boost/numeric/interval/detail/sw64_rounding_control.hpp b/scipy/_lib/boost/boost/numeric/interval/detail/sw64_rounding_control.hpp
new file mode 100644
index 00000000..dd8dbf71
--- /dev/null
+++ b/scipy/_lib/boost/boost/numeric/interval/detail/sw64_rounding_control.hpp
@@ -0,0 +1,93 @@
+/* Boost interval/detail/sw64_rounding_control.hpp file
+ *
+ * Copyright 2005 Felix Höfling, Guillaume Melquiond
+ *
+ * Distributed under the Boost Software License, Version 1.0.
+ * (See accompanying file LICENSE_1_0.txt or
+ * copy at http://www.boost.org/LICENSE_1_0.txt)
+ */
+
+#ifndef BOOST_NUMERIC_INTERVAL_DETAIL_SW64_ROUNDING_CONTROL_HPP
+#define BOOST_NUMERIC_INTERVAL_DETAIL_SW64_ROUNDING_CONTROL_HPP
+
+#if !defined(__sw_64) && !defined(__sw_64__)
+#error This header only works on Sunway CPUs.
+#endif
+
+#if defined(__GNUC__)
+
+#include <float.h> // write_rnd() and read_rnd()
+
+namespace boost {
+namespace numeric {
+namespace interval_lib {
+
+namespace detail {
+    typedef union {
+    ::boost::long_long_type imode;
+    double dmode;
+    } rounding_mode_struct;
+
+    // set bits 59-58 (DYN),
+    // clear all exception bits and disable overflow (51) and inexact exceptions (62)
+    static const rounding_mode_struct mode_upward      = { 0x4C08000000000000LL };
+    static const rounding_mode_struct mode_downward    = { 0x4408000000000000LL };
+    static const rounding_mode_struct mode_to_nearest  = { 0x4808000000000000LL };
+    static const rounding_mode_struct mode_toward_zero = { 0x4008000000000000LL };
+
+    struct sw64_rounding_control
+    {
+    typedef double rounding_mode;
+
+    static void set_rounding_mode(const rounding_mode mode)
+    { __asm__ __volatile__ ("wfpcr %0" : : "f"(mode)); }
+
+    static void get_rounding_mode(rounding_mode& mode)
+    { __asm__ __volatile__ ("rfpcr %0" : "=f"(mode)); }
+
+    static void downward()    { set_rounding_mode(mode_downward.dmode);    }
+    static void upward()      { set_rounding_mode(mode_upward.dmode);      }
+    static void to_nearest()  { set_rounding_mode(mode_to_nearest.dmode);  }
+    static void toward_zero() { set_rounding_mode(mode_toward_zero.dmode); }
+    };
+} // namespace detail
+
+extern "C" {
+  float rintf(float);
+  double rint(double);
+  long double rintl(long double);
+}
+
+template<>
+struct rounding_control<float>:
+  detail::sw64_rounding_control
+{
+  static float force_rounding(const float r)
+  { volatile float _r = r; return _r; }
+  static float to_int(const float& x) { return rintf(x); }
+};
+
+template<>
+struct rounding_control<double>:
+  detail::sw64_rounding_control
+{
+  static const double & force_rounding(const double& r) { return r; }
+  static double to_int(const double& r) { return rint(r); }
+};
+
+template<>
+struct rounding_control<long double>:
+  detail::sw64_rounding_control
+{
+  static const long double & force_rounding(const long double& r) { return r; }
+  static long double to_int(const long double& r) { return rintl(r); }
+};
+
+} // namespace interval_lib
+} // namespace numeric
+} // namespace boost
+
+#undef BOOST_NUMERIC_INTERVAL_NO_HARDWARE
+#endif
+
+#endif /* BOOST_NUMERIC_INTERVAL_DETAIL_SW64_ROUNDING_CONTROL_HPP */
diff --git a/scipy/_lib/boost/boost/predef/architecture/sw64.h b/scipy/_lib/boost/boost/predef/architecture/sw64.h
new file mode 100644
index 00000000..deff8c9a
--- /dev/null
+++ b/scipy/_lib/boost/boost/predef/architecture/sw64.h
@@ -0,0 +1,52 @@
+/*
+Copyright Rene Rivera 2008-2015
+Distributed under the Boost Software License, Version 1.0.
+(See accompanying file LICENSE_1_0.txt or copy at
+http://www.boost.org/LICENSE_1_0.txt)
+*/
+
+#ifndef BOOST_PREDEF_ARCHITECTURE_ALPHA_H
+#define BOOST_PREDEF_ARCHITECTURE_ALPHA_H
+
+#include <boost/predef/version_number.h>
+#include <boost/predef/make.h>
+
+/* tag::reference[]
+= `BOOST_ARCH_SW64`
+
+[options="header"]
+|===
+| {predef_symbol} | {predef_version}
+| `+__sw_64__+` | {predef_detection}
+| `+__sw_64+` | {predef_detection}
+
+| `+__sw_64_sw6a__+` | 1.0.0
+| `+__sw_64_sw6b__+` | 2.0.0
+|===
+*/ // end::reference[]
+
+#define BOOST_ARCH_SW64 BOOST_VERSION_NUMBER_NOT_AVAILABLE
+
+#if defined(__sw_64__) || defined(__sw_64)
+#   undef BOOST_ARCH_SW64
+#   if !defined(BOOST_ARCH_SW64) && defined(__sw_64_sw6a__)
+#       define BOOST_ARCH_SW64 BOOST_VERSION_NUMBER(1,0,0)
+#   endif
+#   if !defined(BOOST_ARCH_SW64) && defined(__sw_64_sw6b__)
+#       define BOOST_ARCH_SW64 BOOST_VERSION_NUMBER(2,0,0)
+#   endif
+#   if !defined(BOOST_ARCH_SW64)
+#       define BOOST_ARCH_SW64 BOOST_VERSION_NUMBER_AVAILABLE
+#   endif
+#endif
+
+#if BOOST_ARCH_SW64
+#   define BOOST_ARCH_SW64_AVAILABLE
+#endif
+
+#define BOOST_ARCH_SW64_NAME "SUNWAY"
+
+#endif
+
+#include <boost/predef/detail/test.h>
+BOOST_PREDEF_DECLARE_TEST(BOOST_ARCH_SW64,BOOST_ARCH_SW64_NAME)
